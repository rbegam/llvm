; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=avx2 | FileCheck %s

; Make sure we switch the order of the zero extends and the shuffles to avoid any shuffles with byte elements.

define internal <8 x i32> @foo(<8 x i8>* %p1, <8 x i8>* %p2, <8 x i8>* %p3, <8 x i8>* %p4) {
; CHECK-LABEL: foo:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vpmovzxbd {{.*#+}} ymm0 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
; CHECK-NEXT:    vpmovzxbd {{.*#+}} ymm1 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
; CHECK-NEXT:    vpmovzxbd {{.*#+}} ymm2 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
; CHECK-NEXT:    vpmovzxbd {{.*#+}} ymm3 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
; CHECK-NEXT:    vperm2i128 {{.*#+}} ymm4 = ymm0[0,1],ymm1[0,1]
; CHECK-NEXT:    vperm2i128 {{.*#+}} ymm5 = ymm2[0,1],ymm3[0,1]
; CHECK-NEXT:    vpsubd %ymm5, %ymm4, %ymm4
; CHECK-NEXT:    vperm2i128 {{.*#+}} ymm0 = ymm0[2,3],ymm1[2,3]
; CHECK-NEXT:    vperm2i128 {{.*#+}} ymm1 = ymm2[2,3],ymm3[2,3]
; CHECK-NEXT:    vpsubd %ymm1, %ymm0, %ymm0
; CHECK-NEXT:    vpslld $16, %ymm0, %ymm0
; CHECK-NEXT:    vpaddd %ymm4, %ymm0, %ymm0
; CHECK-NEXT:    retq
entry:
  %tmp1 = load <8 x i8>, <8 x i8>* %p1, align 1
  %LoadCoalescingShuffle_ = shufflevector <8 x i8> %tmp1, <8 x i8> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %tmp4 = load <8 x i8>, <8 x i8>* %p2, align 1
  %tmp5 = load <8 x i8>, <8 x i8>* %p3, align 1
  %LoadCoalescingShuffle_165 = shufflevector <8 x i8> %tmp4, <8 x i8> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %LoadCoalescingShuffle_167 = shufflevector <8 x i8> %tmp5, <8 x i8> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %LoadCoalescingShuffle_166 = shufflevector <8 x i8> %tmp4, <8 x i8> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %LoadCoalescingShuffle_164 = shufflevector <8 x i8> %tmp1, <8 x i8> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %SplitLoadShuffle129 = shufflevector <4 x i8> %LoadCoalescingShuffle_, <4 x i8> %LoadCoalescingShuffle_165, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %tmp7 = load <8 x i8>, <8 x i8>* %p4, align 1
  %LoadCoalescingShuffle_169 = shufflevector <8 x i8> %tmp7, <8 x i8> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %LoadCoalescingShuffle_170 = shufflevector <8 x i8> %tmp7, <8 x i8> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %SplitLoadShuffle130 = shufflevector <4 x i8> %LoadCoalescingShuffle_167, <4 x i8> %LoadCoalescingShuffle_169, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %SplitLoadShuffle = shufflevector <4 x i8> %LoadCoalescingShuffle_164, <4 x i8> %LoadCoalescingShuffle_166, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %LoadCoalescingShuffle_168 = shufflevector <8 x i8> %tmp5, <8 x i8> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %tmp8 = zext <8 x i8> %SplitLoadShuffle129 to <8 x i32>
  %tmp9 = zext <8 x i8> %SplitLoadShuffle130 to <8 x i32>
  %tmp10 = zext <8 x i8> %SplitLoadShuffle to <8 x i32>
  %SplitLoadShuffle128 = shufflevector <4 x i8> %LoadCoalescingShuffle_168, <4 x i8> %LoadCoalescingShuffle_170, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %tmp11 = zext <8 x i8> %SplitLoadShuffle128 to <8 x i32>
  %tmp12 = sub nsw <8 x i32> %tmp10, %tmp11
  %tmp13 = shl nsw <8 x i32> %tmp12, <i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16>
  %tmp14 = sub nsw <8 x i32> %tmp8, %tmp9
  %tmp15 = add nsw <8 x i32> %tmp13, %tmp14
  ret <8 x i32> %tmp15
}
